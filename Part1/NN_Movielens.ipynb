{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "The goal of this personalization project will be to maximize the accuracy of our recommendation system when considering the results of recommending the top-5 movies for a given user using MovieLens' 10M ratings dataset.\n",
    "\n",
    "Background: MovieLens is a web-based recommender system and virtual community that recommends movies for its users to watch, based on their film preferences using CF of members' movie ratings and reviews. To address the cold-start problem for new users, MovieLens uses preference elicitation where they ask new users to rate how much they enjoy watching different genres of movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 10000054 ratings. Users were selected at random but all users selected had rated at least 20 movies. The data contains three files: \n",
    "\n",
    "#### 1. Movies.dat\n",
    "Each line of this file represents one movie, and has the following format: MovieID::Title::Genres\n",
    "MovieID is the real MovieLens id.\n",
    "\n",
    "Movie titles, by policy, should be entered identically to those found in IMDB, including year of release. However, they are entered manually, so errors and inconsistencies may exist.\n",
    "\n",
    "Genres are a pipe-separated list, and are selected from the following:\n",
    "\n",
    "Action\n",
    "Adventure\n",
    "Animation\n",
    "Children's\n",
    "Comedy\n",
    "Crime\n",
    "Documentary\n",
    "Drama\n",
    "Fantasy\n",
    "Film-Noir\n",
    "Horror\n",
    "Musical\n",
    "Mystery\n",
    "Romance\n",
    "Sci-Fi\n",
    "Thriller\n",
    "War\n",
    "Western\n",
    "\n",
    "\n",
    "#### 2. Ratings.dat\n",
    "All ratings are contained in the file ratings.dat. Each line of this file represents one rating of one movie by one user, and has the following format:\n",
    "\n",
    "UserID::MovieID::Rating::Timestamp\n",
    "\n",
    "The lines within this file are ordered first by UserID, then, within user, by MovieID.\n",
    "\n",
    "Ratings are made on a 1-5 star scale, with half-star increments.\n",
    "\n",
    "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "\n",
    "\n",
    "#### 3. Tags.dat\n",
    "All tags are contained in the file tags.dat. Each line of this file represents one tag applied to one movie by one user, and has the following format:\n",
    "\n",
    "UserID::MovieID::Tag::Timestamp\n",
    "\n",
    "The lines within this file are ordered first by UserID, then, within user, by MovieID.\n",
    "\n",
    "Tags are user generated metadata about movies. Each tag is typically a single word, or short phrase. The meaning, value and purpose of a particular tag is determined by each user.\n",
    "\n",
    "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Item-Based Collaborative Filtering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general methodology we used to create an item-based collaborative filtering algorithm is as follows\n",
    "1. Define a set of mutually observed users that have specified ratings for movie i and j \n",
    "2. Compute similarity between movie i and movie j using Pearson's correlation\n",
    "3. Find top k matching items to movie i for which user u has specified ratings. The weighted average of these ratings is reported as the predicted rating value of that particular item.\n",
    "\n",
    "Our objective is to maximize the accuracy of the top 5 movie recommendations users would enjoy out of those they have not seen yet. To do this we will use an approach that is similar to weighted KNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "moviescol = ['MovieId', 'Title', 'Genres','Action', 'Adventure',\n",
    " 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    " 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "ratings = pd.read_csv('./ratings.dat', sep = '::', names = ['UserId', 'MovieId', 'Rating', 'Timestamp'], engine = 'python')\n",
    "movies = pd.read_csv('./movies.dat', sep ='::', names = moviescol, engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to determine the most appropriate way to sample our data, we first need to get a better understanding of the sparsity level of the user-item matrix. As seen in the introduction paper of item-based CF, we can calculate the Sparsity level of a matrix by using 1- (nonzero entries/total entries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 730 | Number of movies = 6373\n",
      "The sparsity level of MovieLens10M is 97.9%\n"
     ]
    }
   ],
   "source": [
    "# Compute the Sparsity of the Matrix by first finding the number of unique items and users present in the ratings df. Although the Movielens site already provides us with number of unique movies and usrs information, we provide a procedure to obtain it regardless.\n",
    "\n",
    "n_users = ratings['UserId'].nunique()\n",
    "n_items = ratings['MovieId'].nunique()\n",
    "\n",
    "print 'Number of users = ' + str(n_users) + ' | Number of movies = ' + str(n_items)\n",
    "\n",
    "sparsity = round(1.0-len(ratings)/float(n_users*n_items),3)\n",
    "print 'The sparsity level of MovieLens10M is ' +  str(sparsity*100) + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll load a python file in which we define all of our useful functionality. See `funcs.py` in the repository for the source. We use the surprise library for building and training the KNN model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load 'funcs.py'\n",
    "import surprise as sp\n",
    "from surprise import AlgoBase\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cross_validation as cv\n",
    "import funcs as F\n",
    "import itertools as it\n",
    "\n",
    "def build_movie_genre_matrix(movies):\n",
    "    \"\"\"\n",
    "    Build a NxM matrix, rows are movie_ids, columns are genres, values 0, 1\n",
    "    @param movies Dataframe dataframe of movie data\n",
    "    @returns Matrix like Dataframe with 0s and 1s filled in \n",
    "    \"\"\"\n",
    "    movie_genre = []\n",
    "    for (idx, row) in movies.iterrows(): \n",
    "        genres = row.loc['Genres'].split(\"|\")\n",
    "        movieid = row.loc['MovieId']\n",
    "        for g in genres:  \n",
    "            movie_genre.append({'MovieId': movieid, 'Genre': g})\n",
    "\n",
    "    moviegenrecol = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "    test = pd.DataFrame(0, index = np.arange(len(movies)), columns = moviegenrecol)\n",
    "    MovieGenres = pd.concat([movies['MovieId'], test], axis = 1)\n",
    "    MovieGenres.columns= ['MovieId','Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "    for row in movie_genre: \n",
    "        movieID = row['MovieId']\n",
    "        genre = row['Genre']\n",
    "        MovieGenres.loc[MovieGenres.MovieId == movieID, genre] = 1\n",
    "\n",
    "    return MovieGenres\n",
    "\n",
    "        \n",
    "def build_user_item_matrix(ratings):\n",
    "    \"\"\"\n",
    "    Return a USERxITEM matrix with values as the user's value for the movie, null otherwise\n",
    "    Right now not normalized\n",
    "    @param ratings Dataframe\n",
    "    @returns matrix numpy matrix with a user's ratings per movie\n",
    "    \"\"\"\n",
    "    matrix = ratings.pivot(index = 'UserId', columns = 'MovieId', values = 'Rating').fillna(0)\n",
    "    return matrix\n",
    "\n",
    "def sample(ratings, user_counts, movie_counts, n, m):\n",
    "    \"\"\"\n",
    "    Return a smaller matrix with top n users and top m items only\n",
    "    @param ratings the ratings dataset \n",
    "    @param user_counts Count values of user ratings\n",
    "    @param movie_counts Movie count values\n",
    "    @param n number of users with most ratings\n",
    "    @param m number of movies with most ratings\n",
    "    @returns NxM matrix of USERxITEM ratings\n",
    "    \"\"\"\n",
    "    n_users = ratings['UserId'].nunique()\n",
    "    n_items = ratings['MovieId'].nunique()\n",
    "\n",
    "    user_sample = user_counts.head(n).index\n",
    "    movie_sample = movie_counts.head(m).index\n",
    "\n",
    "    print len(user_sample)\n",
    "    print len(movie_sample)\n",
    "\n",
    "    subset = ratings.loc[ratings['UserId'].isin(user_sample)].loc[ratings['MovieId'].isin(movie_sample)]\n",
    "    # we don't need the timestamp\n",
    "    del subset['Timestamp']\n",
    "    return subset\n",
    "\n",
    "\n",
    "# define our own baseline algorithm that Surprise can use. Just predict the average!\n",
    "class AvgBase(AlgoBase):\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "    # see Surprise#building_custom_algo\n",
    "    def train(self, trainset):\n",
    "        AlgoBase.train(self, trainset)\n",
    "        # remember DF is uid, iid, rating\n",
    "        self.mean = np.mean([rating for (_, _, rating) in self.trainset.all_ratings()])\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        return self.mean\n",
    "\n",
    "\n",
    "def normalize_user_means(ratings):\n",
    "    \"\"\"\n",
    "    @param Ratings pandas dataframe\n",
    "    @returns user mean normalized dataframe\n",
    "    \"\"\"\n",
    "    u_i_df = F.build_user_item_matrix(ratings)\n",
    "    means = u_i_df.mean(axis=1)\n",
    "\n",
    "    def f(row):\n",
    "        u_id = row[\"UserId\"]\n",
    "        new_r = row[\"Rating\"] - means[u_id]\n",
    "        return new_r\n",
    "\n",
    "    ratings['Rating'] = ratings.apply(f, axis = 1)\n",
    "    return ratings\n",
    "    \n",
    "\n",
    "# we need something to test our model against. Use above defined model.\n",
    "def train_baseline(ratings):\n",
    "    \"\"\"\n",
    "    Baseline model. Same as below, return a model and data to test it on\n",
    "    @param ratings Pandas Dataframe with UserId, MovieId, Ratings\n",
    "    @returns Tuple (algorithm, testdata) basemodel that just returns the baseline estimate for user/item (adjusted for user bias)\n",
    "    \"\"\"\n",
    "    train_data, test_data = cv.train_test_split(ratings, test_size = 0.20)\n",
    "\n",
    "    algo = AvgBase()\n",
    "    reader = sp.Reader(rating_scale=(1, 5))\n",
    "\n",
    "    trainset = sp.Dataset.load_from_df(ratings, reader)\n",
    "    testset = sp.Dataset.load_from_df(test_data, reader)\n",
    "\n",
    "    trainset = trainset.build_full_trainset()\n",
    "\n",
    "    algo.train(trainset)\n",
    "\n",
    "    testset = testset.build_full_trainset().build_testset()\n",
    "    return (algo, testset)\n",
    "\n",
    "\n",
    "\n",
    "# train the KNN model on subsets of the data (for cross-validation)\n",
    "def train(ratings, k_neighbors, k_folds):\n",
    "    \"\"\"\n",
    "    Train a model and return it. Then we can use the model and evaluate it elsewhere\n",
    "    @param ratings dataframe pandas dataframe to train on, with columns UserId, MovieId, Ratings\n",
    "    @param k_neighbors number of neighbors to examine\n",
    "    @param k_folds number of folds for cross validation\n",
    "    @returns List of (algo, test data)\n",
    "    We can call methods such as `test` and `evaluate` on this object \n",
    "    \"\"\"\n",
    "\n",
    "    train_data, test_data = cv.train_test_split(ratings, test_size = 0.20)\n",
    "    reader = sp.Reader(rating_scale=(1, 5))\n",
    "\n",
    "    trainset = sp.Dataset.load_from_df(train_data, reader)\n",
    "    testset = sp.Dataset.load_from_df(test_data, reader)\n",
    "\n",
    "    trainset.split(n_folds = k_folds)\n",
    "\n",
    "    similarity_options = { 'name': 'pearson', 'user_based': False }\n",
    "    algo = sp.KNNWithMeans(sim_options = similarity_options, k = k_neighbors, min_k = 5)\n",
    "\n",
    "    for _trainset, _ in trainset.folds():\n",
    "        algo.train(_trainset)\n",
    "\n",
    "\n",
    "    testset = testset.build_full_trainset().build_testset()\n",
    "    return (algo, testset)\n",
    "\n",
    "def train_matrix(ratings, factor, k_folds):\n",
    "    \"\"\"\n",
    "    Train a model and return it. Then we can use the model and evaluate it elsewhere\n",
    "    @param ratings dataframe pandas dataframe to train on, with columns UserId, MovieId, Ratings\n",
    "    @param n_folds number of folds for cross validation\n",
    "    @returns List of (algo, test data)\n",
    "    We can call methods such as `test` and `evaluate` on this object \n",
    "    \"\"\"\n",
    "\n",
    "    train_data, test_data = cv.train_test_split(ratings, test_size = 0.20)\n",
    "    reader = sp.Reader(rating_scale=(1, 5))\n",
    "\n",
    "    trainset = sp.Dataset.load_from_df(train_data, reader)\n",
    "    testset = sp.Dataset.load_from_df(test_data, reader)\n",
    "    trainset.split(n_folds = k_folds)\n",
    "\n",
    "    algo = sp.SVD(n_factors = factor)\n",
    "\n",
    "    for trainset, _ in trainset.folds():\n",
    "        algo.train(trainset)\n",
    "        \n",
    "    testset = testset.build_full_trainset().build_testset()\n",
    "    return (algo, testset)\n",
    "\n",
    "\n",
    "def group_predictions_by_user(predictions):\n",
    "    \"\"\"\n",
    "    @param List of Surprise predictions objects\n",
    "    @returns Dict {uid: [P1, P2, ...PN]} hash mapping user id to top n predictions\n",
    "    \"\"\"\n",
    "    p = sorted(predictions, key = lambda x: x.uid)\n",
    "\n",
    "    groups = {}\n",
    "    for k, g in it.groupby(p, lambda x: x.uid):\n",
    "        groups[k] = sorted(list(g), key = lambda x: x.est, reverse = True)\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "# For every item that a user would be rated, in top k\n",
    "def calculate_catalog_coverage(ratings, predictions, k):\n",
    "    \"\"\"\n",
    "    Calculate the catalog coverage of a model over a dataset\n",
    "    @param ratings pandas dataframe with UserId, MovieId, Ratings. Must be the same set the model was trained on\n",
    "    @param List Surprise predictions\n",
    "    @oaram k Int the top k recommendations size\n",
    "    @returns Float percentage of items recommended to at least one user\n",
    "    \"\"\"\n",
    "    n_movies = ratings['MovieId'].nunique()\n",
    "\n",
    "    movies_reccommended = set() # keep track of which movies are recommended. Note we only care about the number\n",
    "\n",
    "    recommendations = group_predictions_by_user(predictions)\n",
    "    for u_id, recs in recommendations.iteritems():\n",
    "        movies_reccommended.update(map(lambda x: x.iid, recs[0:3]))\n",
    "\n",
    "    return len(movies_reccommended) / float(n_movies)\n",
    "\n",
    "\n",
    "# print some evaluative summaries\n",
    "def evaluate(algo, ratings, testset, top_k = 5):\n",
    "    \"\"\"\n",
    "    @param algo Surprise algorithm the model that was trained\n",
    "    @oaram ratings The ratings it was trained on, in pandas Dataframe form (so we can calculate coverage)\n",
    "    @param testset Surprise testset object, the data held out during cross-validation\n",
    "    @returns Nested Dictionary {test: {rmse, mae}, train: {rmse, mae, cc}}\n",
    "    We can use these to build up arrays for plotting.\n",
    "    \"\"\"\n",
    "\n",
    "    ret = {}\n",
    "    ret['test'] = {}\n",
    "    ret['train'] = {}\n",
    "\n",
    "    test_predictions = algo.test(testset)\n",
    "    # see how it would do on the trainset to compare, comes with the algo object\n",
    "    trainset = algo.trainset.build_testset()\n",
    "    train_predictions = algo.test(trainset)\n",
    "\n",
    "    # sticking evaluate in everything for grep, training is verbose\n",
    "    ret['test']['rmse'] = sp.accuracy.rmse(test_predictions)\n",
    "    ret['train']['rmse'] = sp.accuracy.rmse(train_predictions)\n",
    "\n",
    "    ret['test']['mae'] = sp.accuracy.mae(test_predictions)\n",
    "    ret['train']['mae'] = sp.accuracy.mae(train_predictions)\n",
    "\n",
    "    # Hackish, baseline does not have a sense of \"neighbors\"\n",
    "    knn = algo.__module__ = \"surprise.prediction_algorithms.knns\"\n",
    "    mf =  algo.__module__ = \"surprise.prediction_algorithms.matrix_factorization\"\n",
    "\n",
    "    if (knn or mf):\n",
    "        ret['test']['cc'] = calculate_catalog_coverage(ratings, test_predictions, top_k)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# matrix = funcs.build_user_item_matrix(ratings, movies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we want to work with a sample size of 10000 users and 100 items to start with, we cannot sample at random due to the high sparsity level previously calculated. Therefore we will obtain the top 10000 users with the most ratings and the most rated 100 movies. Funcs.py defines just such a sampling function, taking the top n users with the most ratings and the top-m most rated movies. In order to sample we need to know the top ratings N users and top rated M movies respectively. We can do this inside the sampling function, but since this doesn't change across sample sizes, we'll do it once outside the function and pass the value counts in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_counts = ratings['UserId'].value_counts()\n",
    "movie_counts = ratings['MovieId'].value_counts()\n",
    "\n",
    "subset = sample(ratings, user_counts, movie_counts, 10000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can train our model using a function defined above. `train(data, k_neighbors, k_folds)` handles splitting the data into train and test sets, and folds for cross validation. This function trains our model, and then returns to us a tuple containing the (model, testdata) i.e. the model and the 20% of test data that we have withheld, so that way can test it and do what we like with the predictions. \n",
    "\n",
    "The library calculates mean centered pearson similarity under the hood. Conceptually it is simply mean centering all of the items, and computing the pairwise similarity for each, based on user ratings. This is the slowest part of the model, running in `O(nm^2)` time, where m is the number of items and n is the number of users. This is the reason that item-based methods are more tractable than user-based methods; the number of items is usually far less than the number of users. `nm^2` is still pretty big, but since this gets run offline that's alright, as long as it runs on the order of hours rather than weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "model, testdata = train(subset, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our model. The function defined above `evaluate(algo, rating, testset, top_k=5)` takes the trained model, the original dataset, a testset, and the number of top-k recs and comptues some useful summaries about our model. Specifically it returns a hash with RMSE, MAE for test and train sets, and catalog coverage for the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8508\n",
      "RMSE: 0.5245\n",
      "MAE:  0.6501\n",
      "MAE:  0.4068\n"
     ]
    }
   ],
   "source": [
    "evaluation = evaluate(model, subset, testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': {'cc': 0.99, 'mae': 0.65010674683631375, 'rmse': 0.85077590085280186}, 'train': {'mae': 0.40683865954621445, 'rmse': 0.52454923642969709}}\n"
     ]
    }
   ],
   "source": [
    "print evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Based CF Algorithm\n",
    "Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation Methods\n",
    "\n",
    "## (i) Cross Validation Setup\n",
    "  The `surprise` library handles cross validation for us. After we split the ratings dataframe into train, and test (80%, 20%), `surprise` gives you the option to split the training data into different folds. We can iterate over these folds and train successively. This ensures that we're training over a balanced set. Finally, we withhold the test data from training and return it along with the model so that we can test it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## (ii) Accuracy on Training/Test Data\n",
    "For accuracy we can take a look at MAE and RMSE of the models over a baseline. Our `evaluate` method returns a hash with the mae, rmse for the test and train predictions. We ran our models (offline) across a wide range of hyperparameters, using k-values and latent factors of [5, 10, 15...55]. Some of the results are plotted below. \n",
    "\n",
    "The errors are much smaller on training data for obvious reasons, but even on test data we beat the baseline by a large amount. MAEs were as low as 0.50 for some iterations of KNN and MF on the subset with 10,000 users and 2,000 items. For reference, the baseline, the simple average across all ratings, was around 0.80 for most runs.\n",
    "\n",
    "The code used to generate the offline predictions appears below, taken from `script.py` in the repository. If one wants to run the model, simply run `python script.py` in the shell. This will take several hours if the larger subsets are included.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the neighborhood has significant impact on the prediction quality. So to determine the sensitivity of this parameter, we varied the number of neighbors used and observed its effect on MAE. Our results are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#MAE vs Neighborhood size\n",
    "plt.plot(k, Sample1MAE, k, Sample2MAE, k, Sample3MAE)\n",
    "plt.title('Sensitivity of the Neighborhood Size')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-56862f28282e>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-56862f28282e>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    test = pd.read_json(../forKathyKnnVsErr.json, orient = 'index')\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "test = pd.read_json(/forKathyKnnVsErr.json, orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity, the number of latent factors used for our matrix factorization impacts our prediction quality as well. We analyzed the sensitivity of this parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MAE vs Neighborhood size\n",
    "plt.plot(f, Sample1MAE, f, Sample2MAE, f, Sample3MAE)\n",
    "plt.title('Sensitivity of the Neighborhood Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## (iii) Coverage on training and test data\n",
    "\ta. User-Coverage: the fraction of users for which AT LEAST k items can be recommended well \n",
    "\tb. Item-Coverage: the fraction of items that can be recommended to at least k users well\n",
    "\tc. Catalog Coverage: the fraction of items that are in the top-k for at least 1 user\n",
    "    \n",
    "\n",
    "\n",
    "How do your evaluation metrics change as a function of parameters such as neighborhood size, # of latent dimensions? \n",
    "\n",
    "_Personal Note_ In general when the neighborhood size K is small, we're forcing our classifier to be \"more blind\" to the overall distribution. A small K will have low bias but higher variance. On the other hand, a higher K averages more voters in each prediction and is more resilient to outliers. This consequently results in lower variance but increased bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Size Variation\n",
    "How does overall accuracy change when you systematically sample your data from a small to large size? How does runtime scale? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As mentioned earlier, intuitively we'd expect the runtime of both algorithms to increase as our model size increases. The figure below validates this intuitoin and compares the runtime of our Item Based model against its model based counterpart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MAE vs Neighborhood size\n",
    "plt.plot(k, Sample1MAE, k, Sample2MAE, k, Sample3MAE)\n",
    "plt.title('Sensitivity of the Neighborhood Size')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
