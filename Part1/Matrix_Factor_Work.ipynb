{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import surprise as sp\n",
    "from surprise import Dataset, Reader, SVD, evaluate\n",
    "from sklearn import cross_validation as cv\n",
    "\n",
    "ratingscol = ['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
    "ratings = pd.read_csv('./ratings100k.dat', sep='::', names = ratingscol, engine='python')\n",
    "ratings = ratings.drop('Timestamp', axis=1)\n",
    "\n",
    "def sample(ratings, n, m):\n",
    "    \"\"\"\n",
    "    Return a smaller matrix with top n users and top m items only\n",
    "    @param ratings the ratings dataset \n",
    "    @param n number of users with most ratings\n",
    "    @param m number of movies with most ratings\n",
    "    @returns NxM matrix of USERxITEM ratings\n",
    "    \"\"\"\n",
    "\n",
    "    n_users = ratings['UserID'].nunique()\n",
    "    n_items = ratings['MovieID'].nunique()\n",
    "\n",
    "    user_sample = ratings['UserID'].value_counts().head(n).index\n",
    "    movie_sample = ratings['MovieID'].value_counts().head(m).index\n",
    "\n",
    "    subset = ratings.loc[ratings['UserID'].isin(user_sample)].loc[ratings['MovieID'].isin(movie_sample)]\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_matrix(ratings, factor):\n",
    "    \"\"\"\n",
    "    Train a model and return it. Then we can use the model and evaluate it elsewhere\n",
    "    @param ratings dataframe pandas dataframe to train on, with columns UserId, MovieId, Ratings\n",
    "    @param n_folds number of folds for cross validation\n",
    "    @returns List of (algo, test data)\n",
    "    We can call methods such as `test` and `evaluate` on this object \n",
    "    \"\"\"\n",
    "\n",
    "    train_data, test_data = cv.train_test_split(ratings, test_size = 0.20)\n",
    "    reader = sp.Reader(rating_scale=(1, 5))\n",
    "\n",
    "    trainset = sp.Dataset.load_from_df(train_data, reader)\n",
    "    testset = sp.Dataset.load_from_df(test_data, reader)\n",
    "    trainset.split(n_folds = 5)\n",
    "\n",
    "    algo = sp.SVD(n_factors = factor)\n",
    "\n",
    "    for trainset, _ in trainset.folds():\n",
    "        algo.train(trainset)\n",
    "        \n",
    "    testset = testset.build_full_trainset().build_testset()\n",
    "    return (algo, testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_predictions_by_user(predictions):\n",
    "    \"\"\"\n",
    "    @param List of Surprise predictions objects\n",
    "    @returns Dict {uid: [P1, P2, ...PN]} hash mapping user id to top n predictions\n",
    "    \"\"\"\n",
    "    p = sorted(predictions, key = lambda x: x.uid)\n",
    "\n",
    "    groups = {}\n",
    "    for k, g in it.groupby(p, lambda x: x.uid):\n",
    "        groups[k] = sorted(list(g), key = lambda x: x.est, reverse = True)\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "# For every item that a user would be rated, in top k\n",
    "def calculate_catalog_coverage(ratings, predictions, k):\n",
    "    \"\"\"\n",
    "    Calculate the catalog coverage of a model over a dataset\n",
    "    @param ratings pandas dataframe with UserId, MovieId, Ratings. Must be the same set the model was trained on\n",
    "    @param List Surprise predictions\n",
    "    @oaram k Int the top k recommendations size\n",
    "    @returns Float percentage of items recommended to at least one user\n",
    "    \"\"\"\n",
    "    n_movies = ratings['MovieId'].nunique()\n",
    "\n",
    "    movies_reccommended = set() # keep track of which movies are recommended. Note we only care about the number\n",
    "\n",
    "    recommendations = group_predictions_by_user(predictions)\n",
    "    for u_id, recs in recommendations.iteritems():\n",
    "        movies_reccommended.update(map(lambda x: x.iid, recs[0:3]))\n",
    "\n",
    "    return len(movies_reccommended) / float(n_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(algo, ratings, testset, top_k):\n",
    "    \"\"\"\n",
    "    @param algo Surprise algorithm the model that was trained\n",
    "    @oaram ratings The ratings it was trained on, in pandas Dataframe form (so we can calculate coverage)\n",
    "    @param testset Surprise testset object, the data held out during cross-validation\n",
    "    @returns Nested Dictionary {test: {rmse, mae}, train: {rmse, mae, cc}}\n",
    "    We can use these to build up arrays for plotting.\n",
    "    \"\"\"\n",
    "\n",
    "    ret = {}\n",
    "    ret['test'] = {}\n",
    "    ret['train'] = {}\n",
    "\n",
    "    test_predictions = algo.test(testset)\n",
    "    # see how it would do on the trainset to compare, comes with the algo object\n",
    "    trainset = algo.trainset.build_testset()\n",
    "    train_predictions = algo.test(trainset)\n",
    "\n",
    "    # sticking evaluate in everything for grep, training is verbose\n",
    "    ret['test']['rmse'] = sp.accuracy.rmse(test_predictions)\n",
    "    ret['train']['rmse'] = sp.accuracy.rmse(train_predictions)\n",
    "\n",
    "    ret['test']['mae'] = sp.accuracy.mae(test_predictions)\n",
    "    ret['train']['mae'] = sp.accuracy.mae(train_predictions)\n",
    "\n",
    "    # Hackish, baseline does not have a sense of \"neighbors\"\n",
    "    if (algo.__module__ == \"surprise.prediction_algorithms.knns\"):\n",
    "        ret['test']['cc'] = calculate_catalog_coverage(ratings, test_predictions, top_k)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MF with f of 5\n",
      "RMSE: 0.8913\n",
      "RMSE: 0.7613\n",
      "MAE:  0.6980\n",
      "MAE:  0.5935\n",
      "Evaluating MF with f of 10\n",
      "RMSE: 0.8181\n",
      "RMSE: 0.7762\n",
      "MAE:  0.6593\n",
      "MAE:  0.6065\n",
      "Evaluating MF with f of 20\n",
      "RMSE: 0.8875\n",
      "RMSE: 0.7449\n",
      "MAE:  0.6809\n",
      "MAE:  0.5851\n",
      "Evaluating MF with f of 40\n",
      "RMSE: 0.8224\n",
      "RMSE: 0.7292\n",
      "MAE:  0.6640\n",
      "MAE:  0.5719\n",
      "Evaluating MF with f of 50\n",
      "RMSE: 0.8981\n",
      "RMSE: 0.7027\n",
      "MAE:  0.7031\n",
      "MAE:  0.5516\n",
      "Evaluating MF with f of 75\n",
      "RMSE: 0.9062\n",
      "RMSE: 0.6469\n",
      "MAE:  0.6933\n",
      "MAE:  0.5098\n",
      "Evaluating MF with f of 5\n",
      "RMSE: 0.8463\n",
      "RMSE: 0.7787\n",
      "MAE:  0.6609\n",
      "MAE:  0.6074\n",
      "Evaluating MF with f of 10\n",
      "RMSE: 0.8495\n",
      "RMSE: 0.7635\n",
      "MAE:  0.6596\n",
      "MAE:  0.5983\n",
      "Evaluating MF with f of 20\n",
      "RMSE: 0.8384\n",
      "RMSE: 0.7609\n",
      "MAE:  0.6531\n",
      "MAE:  0.5942\n",
      "Evaluating MF with f of 40\n",
      "RMSE: 0.8476\n",
      "RMSE: 0.7268\n",
      "MAE:  0.6625\n",
      "MAE:  0.5658\n",
      "Evaluating MF with f of 50\n",
      "RMSE: 0.8664\n",
      "RMSE: 0.7033\n",
      "MAE:  0.6809\n",
      "MAE:  0.5475\n",
      "Evaluating MF with f of 75\n",
      "RMSE: 0.8519\n",
      "RMSE: 0.6598\n",
      "MAE:  0.6638\n",
      "MAE:  0.5152\n",
      "Evaluating MF with f of 5\n",
      "RMSE: 0.8515\n",
      "RMSE: 0.7945\n",
      "MAE:  0.6634\n",
      "MAE:  0.6178\n",
      "Evaluating MF with f of 10\n",
      "RMSE: 0.8410\n",
      "RMSE: 0.7872\n",
      "MAE:  0.6494\n",
      "MAE:  0.6135\n",
      "Evaluating MF with f of 20\n",
      "RMSE: 0.8573\n",
      "RMSE: 0.7607\n",
      "MAE:  0.6581\n",
      "MAE:  0.5927\n",
      "Evaluating MF with f of 40\n",
      "RMSE: 0.8399\n",
      "RMSE: 0.7285\n",
      "MAE:  0.6601\n",
      "MAE:  0.5642\n",
      "Evaluating MF with f of 50\n",
      "RMSE: 0.8532\n",
      "RMSE: 0.7109\n",
      "MAE:  0.6641\n",
      "MAE:  0.5536\n",
      "Evaluating MF with f of 75\n",
      "RMSE: 0.8485\n",
      "RMSE: 0.6736\n",
      "MAE:  0.6586\n",
      "MAE:  0.5248\n",
      "Evaluating MF with f of 5\n",
      "RMSE: 0.8286\n",
      "RMSE: 0.8002\n",
      "MAE:  0.6459\n",
      "MAE:  0.6217\n",
      "Evaluating MF with f of 10\n",
      "RMSE: 0.8469\n",
      "RMSE: 0.7920\n",
      "MAE:  0.6552\n",
      "MAE:  0.6151\n",
      "Evaluating MF with f of 20\n",
      "RMSE: 0.8266\n",
      "RMSE: 0.7711\n",
      "MAE:  0.6470\n",
      "MAE:  0.5972\n",
      "Evaluating MF with f of 40\n",
      "RMSE: 0.8311\n",
      "RMSE: 0.7357\n",
      "MAE:  0.6465\n",
      "MAE:  0.5720\n",
      "Evaluating MF with f of 50\n",
      "RMSE: 0.8527\n",
      "RMSE: 0.7191\n",
      "MAE:  0.6642\n",
      "MAE:  0.5580\n",
      "Evaluating MF with f of 75\n",
      "RMSE: 0.8435\n",
      "RMSE: 0.6811\n",
      "MAE:  0.6525\n",
      "MAE:  0.5300\n"
     ]
    }
   ],
   "source": [
    "# run models with some different parameters and sizes\n",
    "# samples = [ [1000, 10], [5000, 50], [100000, 1000], [5000000, 2000] ]\n",
    "samples = [ [1000, 10], [5000, 50], [10000,100], [20000,200] ]\n",
    "factors = [5, 10, 20, 40, 50, 75]\n",
    "\n",
    "for _sample in samples:\n",
    "    i, j = _sample\n",
    "    _dataset = sample(ratings, i, j)\n",
    "    for f in factors:\n",
    "        MF, MF_test = train_matrix(_dataset, f)\n",
    "\n",
    "        print \"Evaluating MF with f of {}\".format(f)\n",
    "        evaluate(MF, _dataset, MF_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
